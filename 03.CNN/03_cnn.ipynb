{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages we'll use\n",
    "\n",
    "import numpy as np\n",
    "import os, glob, csv\n",
    "\n",
    "# librosa is a widely-used audio processing library\n",
    "import librosa\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnF\n",
    "\n",
    "# for plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "# for accuracy and confusion matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# for data normalization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# use GPU if available, otherwise, use cpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER CONFIGURATION\n",
    "# Please alter the paths here to where the data are stored on your local filesystem\n",
    "binarylabelcsv  = os.path.expanduser(\"data/warblrb10k_public_metadata_2018.csv\")\n",
    "audiofilefolder = os.path.expanduser(\"data/warblrb10k_public_wav\")\n",
    "\n",
    "# we experiment with 100 files here. In practice, it depends on your actual training, validation, and test data\n",
    "#maxfilestoload  = 1000      # limit, because loading the whole dataset is very slow\n",
    "maxfilestoload  = 100      # limit, because loading the whole dataset is very slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('000e088a-69f7-4d7a-ba7b', 0.0)\n",
      "('00a29637-99aa-4f23-97c9', 1.0)\n",
      "('00a91d50-1b15-480a-b533', 0.0)\n",
      "('00c113b9-7156-4b34-8d50', 1.0)\n",
      "('00cc9afb-40da-4ca3-a4fe', 1.0)\n",
      "('00d14a65-1ccd-48e1-8dbf', 0.0)\n",
      "('00e10c18-3dae-4303-bfe2', 1.0)\n",
      "('01a0e0a4-ce8e-4e19-8cab', 1.0)\n",
      "('01dda7d0-fff5-4c87-bfaf', 1.0)\n",
      "('0a3c5d54-bf32-4a1d-aab4', 1.0)\n",
      "('0a4ef72d-611f-4adc-9cf1', 1.0)\n"
     ]
    }
   ],
   "source": [
    "# here we load the metadata labels\n",
    "wavs = []\n",
    "for wav in os.scandir(audiofilefolder):\n",
    "    wavs.append(wav.name[0:-4])\n",
    "binarylabels = {}\n",
    "with open(binarylabelcsv, 'r') as infp:\n",
    "        rdr = csv.DictReader(infp)\n",
    "        \n",
    "        for row in rdr:\n",
    "            if row['itemid'] in wavs:\n",
    "                binarylabels[row['itemid']] = float(row['hasbird'])\n",
    "            if len(binarylabels)==maxfilestoload:\n",
    "                    break  # note, here we are restricting the maximum number of rows.\n",
    "\n",
    "fkeys = sorted(binarylabels.keys())\n",
    "# inspect:\n",
    "for i, kv in enumerate(binarylabels.items()):\n",
    "    print(kv)\n",
    "    if i==10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 000e088a-69f7-4d7a-ba7b\n",
      "1: 00a29637-99aa-4f23-97c9\n",
      "2: 00a91d50-1b15-480a-b533\n",
      "3: 00c113b9-7156-4b34-8d50\n",
      "4: 00cc9afb-40da-4ca3-a4fe\n",
      "5: 00d14a65-1ccd-48e1-8dbf\n",
      "6: 00e10c18-3dae-4303-bfe2\n",
      "7: 01a0e0a4-ce8e-4e19-8cab\n",
      "8: 01dda7d0-fff5-4c87-bfaf\n",
      "9: 0a3c5d54-bf32-4a1d-aab4\n",
      "10: 0a4ef72d-611f-4adc-9cf1\n",
      "11: 0a66707d-b68c-43ed-a681\n",
      "12: 0a9a74b9-18ec-4c35-9c59\n",
      "13: 0aa1052a-827d-4afe-a4e7\n",
      "14: 0aa146f7-2230-4a15-acc6\n",
      "15: 0aa67c37-5fc9-48a1-b782\n",
      "16: 0ac08694-de66-4947-895f\n",
      "17: 0afa6817-1ae9-4575-9763\n",
      "18: 0b5f3061-6838-4e2b-9bdd\n",
      "19: 0b755770-cac3-4962-bc8f\n",
      "20: 0b7d8a53-ceea-4050-976f\n",
      "21: 0b856a74-0d5b-44b5-a204\n",
      "22: 0b9fb0ec-a78e-4f4a-9545\n",
      "23: 0bfc4f9b-0a47-45ea-b41f\n",
      "24: 0c37de0f-b8ba-4823-81c4\n",
      "25: 0c3b9ff3-7abf-4db2-a1c2\n",
      "26: 0c603a84-2e93-47d1-af3d\n",
      "27: 0ca97190-963b-4be9-bead\n",
      "28: 0ce55600-24ea-419f-95a2\n",
      "29: 0d30dbf2-dfdc-4e46-bce3\n",
      "30: 0d534723-5714-4493-8ccd\n",
      "31: 0d5e3cdc-9038-452b-baf4\n",
      "32: 0d6e09ad-1412-4259-9949\n",
      "33: 0d703f38-2c38-438a-9cd2\n",
      "34: 0d787428-923d-4b95-9496\n",
      "35: 0d8d91c8-a985-47b5-b6f8\n",
      "36: 0d900867-9d6f-4b18-9300\n",
      "37: 0dae4b19-89be-4c5a-9d8a\n",
      "38: 0dbc306e-af99-427c-9955\n",
      "39: 0dca3b63-1cfb-4ae1-8113\n",
      "40: 0dd0b7bd-4fb7-48da-8334\n",
      "41: 0df3e645-0e0b-4c91-a0a9\n",
      "42: 0e06e796-f4cd-40cd-879a\n",
      "43: 0e61f908-eb56-469d-953a\n",
      "44: 0e74e552-9cbc-4330-8a6e\n",
      "45: 0e847bd2-fe47-40cf-9f11\n",
      "46: 0eacbedf-b03b-47a4-8e71\n",
      "47: 0ee535c1-7a6b-42d7-94b2\n",
      "48: 0f223afb-205a-4e80-a5d1\n",
      "49: 0f446505-9447-4e06-af4f\n",
      "50: 0f51b95c-1a1f-4597-81d3\n",
      "51: 0f6dff41-e0b7-4658-8007\n",
      "52: 0fed552e-e9b3-41ef-bffd\n",
      "53: 1a01ceda-65d3-4c7a-bdf7\n",
      "54: 1a2d1049-a594-42b8-8b00\n",
      "55: 1a4c4f65-3b3e-40bd-bdf1\n",
      "56: 1a68202f-3647-4da0-8cef\n",
      "57: 1a7c332a-1e5f-4a95-b6a2\n",
      "58: 1aa9e093-4e3f-42cf-b5ce\n",
      "59: 1ab9d3e5-ff89-4ea7-a455\n",
      "60: 1b3854ff-17ec-48c6-89ab\n",
      "61: 1b47dede-fd9d-4d7b-8d10\n",
      "62: 1b66bd3b-096a-4860-91f8\n",
      "63: 1b98927c-c33e-4c49-b27b\n",
      "64: 1bb263fc-6861-4745-a39a\n",
      "65: 1bb6e700-bed0-41e0-980b\n",
      "66: 1bd6d2a6-c360-4991-835c\n",
      "67: 1c027640-d8b4-401f-8f7b\n",
      "68: 1c2fa941-02e0-43e3-84cc\n",
      "69: 1c5fca57-a297-4d88-8d90\n",
      "70: 1c637571-ae1f-4a44-b2de\n",
      "71: 1c797793-1849-4df5-8eda\n",
      "72: 1c7df876-d435-45bd-bde5\n",
      "73: 1d1bb98a-1fdb-4c96-ab59\n",
      "74: 1d8ca5c9-008c-4756-aa05\n",
      "75: 1d93352c-d2a3-4079-a294\n",
      "76: 1d94fc4a-1c63-4da0-9cac\n",
      "77: 1db06bb1-b4a9-4fe4-8a22\n",
      "78: 1db2f8e4-a209-4ea3-8aa2\n",
      "79: 1dbc5fce-afcb-4656-a354\n",
      "80: 1e01b0e9-7f54-4c07-b766\n",
      "81: 1e042095-0b5e-4041-84b0\n",
      "82: 1e069b5a-047e-4ad6-b07a\n",
      "83: 1e860c93-dd38-4711-88de\n",
      "84: 1e8708b8-f6f5-4ebe-b718\n",
      "85: 1eba20cb-5e29-4f47-8bcb\n",
      "86: 1ebb7995-d3f7-4d19-a455\n",
      "87: 1ec9ff90-7f34-44ff-98cd\n",
      "88: 1ed79c6c-b714-48ae-b21a\n",
      "89: 1eeced6b-1ac6-4dd7-a9d4\n",
      "90: 1f2bbf69-6785-4211-81b8\n",
      "91: 1f453430-7958-4e1e-ab89\n",
      "92: 1f50320e-a671-49cc-af6a\n",
      "93: 1f78c892-5eb1-4d14-a1e8\n",
      "94: 1f9248c3-c4f8-4a1e-b5b0\n",
      "95: 1f92a18b-9a8e-424c-a06e\n",
      "96: 1fa029bd-6b66-484e-90b3\n",
      "97: 1fa3a2f5-3c26-4581-aeea\n",
      "98: 1fc1914b-2409-4e08-ba86\n",
      "99: 1fd5549f-116e-44b6-946b\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- Load an example audio file, converting the audio data to mel spectrogram\n",
    "- window length 50 ms, hop_len 25 ms\n",
    "'''\n",
    "def extract_melspectrogram(filename, win_len=0.05, hop_len=0.025, n_mels=64):\n",
    "    audio, sr = librosa.load(\"%s/%s.wav\" % (audiofilefolder, filename), sr=22050)\n",
    "    win_len = int(win_len*sr)\n",
    "    hop_len = int(hop_len*sr)\n",
    "    spec = librosa.feature.melspectrogram(audio, sr, n_mels=n_mels, n_fft=2048, win_length=win_len, hop_length=hop_len)\n",
    "    # return data format (time_len, n_mels)\n",
    "    return spec.transpose((1,0))\n",
    "'''\n",
    " - Load the data, \n",
    " - Extract mel spectrograms\n",
    " - Annotation: one element corresponding to one audio file\n",
    "'''\n",
    "data = np.zeros((maxfilestoload, 400, 64)) # for storing mel spectrograms\n",
    "label = np.zeros(maxfilestoload) # for storing the annotion\n",
    "for i, kv in enumerate(binarylabels.items()):\n",
    "    print(f'{i}: {kv[0]}')\n",
    "    # the number of the melspectrograms' time frames varies a bit (due to some small differences in audio length)\n",
    "    # for simplicity, let's take a maximum of 400 time frames.\n",
    "    mel = extract_melspectrogram(kv[0])\n",
    "    if len(mel)>=400:\n",
    "        data[i] = mel[:400] \n",
    "    else:\n",
    "        data[i] = np.pad(mel, ((0, 400-len(mel)), (0,0)), 'constant')\n",
    "    label[i] = kv[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 400, 64)\n",
      "(10, 400, 64)\n",
      "(10, 400, 64)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- Split the data into \n",
    "    training (80%)\n",
    "    validation (10%)\n",
    "    test (10%)\n",
    "'''\n",
    "#print(label)\n",
    "#print(data.shape)\n",
    "#print(data[0])\n",
    "\n",
    "# training data\n",
    "train_data = data[:int(0.8*maxfilestoload)]\n",
    "train_label = label[:int(0.8*maxfilestoload)]\n",
    "print(train_data.shape)\n",
    "\n",
    "# validation data\n",
    "valid_data = data[int(0.8*maxfilestoload):int(0.9*maxfilestoload)]\n",
    "valid_label = label[int(0.8*maxfilestoload):int(0.9*maxfilestoload)]\n",
    "print(valid_data.shape)\n",
    "\n",
    "# test data\n",
    "test_data = data[int(0.9*maxfilestoload):]\n",
    "test_label = label[int(0.9*maxfilestoload):]\n",
    "print(test_data.shape)\n",
    "\n",
    "#del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.27417198e-02 6.71528475e-01 3.34884857e+00 5.34982430e+00\n",
      " 3.93831190e+00 2.61192580e+00 1.70658188e+00 1.52840656e+00\n",
      " 1.85668432e+00 1.45171007e+00 1.15428010e+00 1.11384099e+00\n",
      " 7.23883607e-01 7.04022287e-01 9.02137238e-01 9.09713848e-01\n",
      " 6.87939900e-01 6.16596413e-01 5.98780611e-01 6.16074710e-01\n",
      " 5.68590739e-01 5.25516220e-01 5.17564556e-01 4.45009597e-01\n",
      " 4.24881280e-01 4.42419693e-01 5.18095678e-01 7.07851586e-01\n",
      " 3.90019947e-01 3.88621509e-01 4.21898749e-01 3.66527371e-01\n",
      " 4.00831815e-01 4.93162278e-01 3.22484143e-01 3.38480112e-01\n",
      " 2.71392352e-01 1.37921975e-01 1.20895857e-01 1.68032295e-01\n",
      " 1.95311585e-01 1.97136537e-01 1.95864490e-01 1.93853594e-01\n",
      " 2.46105523e-01 2.64727193e-01 2.01721641e-01 1.57078354e-01\n",
      " 1.12280165e-01 7.69656697e-02 7.92673513e-02 4.89091927e-02\n",
      " 3.70301325e-02 4.30410108e-02 3.20120648e-02 2.51270714e-02\n",
      " 2.97772166e-02 4.03291139e-02 2.83574592e-02 1.33856931e-02\n",
      " 5.66312032e-03 3.77323130e-03 2.96299886e-03 9.58404805e-04]\n"
     ]
    }
   ],
   "source": [
    "# data normalisation\n",
    "scaler = StandardScaler()\n",
    "# compute normalisation parameters based on the training data \n",
    "# QUESTION: why do we reshape the data to (-1,64)?\n",
    "# A: Compress the time data to calculate the mean\n",
    "scaler.fit(train_data.reshape((-1,64)))\n",
    "print(scaler.mean_)\n",
    "\n",
    "# normalise the training data with the computed parameters\n",
    "train_data = scaler.transform(train_data.reshape((-1,64)))\n",
    "train_data = train_data.reshape((-1, 400, 64)) # reverse back to the original shape\n",
    "#print(train_data[0])\n",
    "\n",
    "# normalise the validation data with the computed parameters\n",
    "valid_data = scaler.transform(valid_data.reshape((-1,64)))\n",
    "valid_data = valid_data.reshape((-1, 400, 64)) # reverse back to the original shape\n",
    "#print(valid_data[0])\n",
    "\n",
    "# normalise the test data with the computed parameters\n",
    "test_data = scaler.transform(test_data.reshape((-1,64)))\n",
    "test_data = test_data.reshape((-1, 400, 64)) # reverse back to the original shape\n",
    "#print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    \"\"\"Initialize a Linear or Convolutional layer. \n",
    "    Ref: He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing \n",
    "    human-level performance on imagenet classification.\" Proceedings of the \n",
    "    IEEE international conference on computer vision. 2015.\n",
    "    \"\"\"\n",
    "    \n",
    "    if layer.weight.ndimension() == 4:\n",
    "        (n_out, n_in, height, width) = layer.weight.size()\n",
    "        n = n_in * height * width\n",
    "        \n",
    "    elif layer.weight.ndimension() == 2:\n",
    "        (n_out, n) = layer.weight.size()\n",
    "\n",
    "    std = math.sqrt(2. / n)\n",
    "    scale = std * math.sqrt(3.)\n",
    "    layer.weight.data.uniform_(-scale, scale)\n",
    "\n",
    "    if layer.bias is not None:\n",
    "        layer.bias.data.fill_(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bn(bn):\n",
    "    \"\"\"Initialize a Batchnorm layer. \"\"\"\n",
    "    \n",
    "    bn.weight.data.fill_(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnModel(nn.Module):\n",
    "    \"\"\"The CNN model\"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(CnnModel, self).__init__()\n",
    "        \n",
    "        # FILLING THE ... TO COMPLETE THE 1ST 2D CONV LAYER OF THE NETWORK GIVEN:\n",
    "        # - kernel size 5x5\n",
    "        # - the number of kernels: 64\n",
    "        # What is the value of in_channels here?\n",
    "        # Ref: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, \n",
    "                               out_channels=64,\n",
    "                               kernel_size=(5,5), \n",
    "                               bias=False)\n",
    "\n",
    "        # FILLING THE ... TO COMPLETE THE 2ST 2D CONV LAYER OF THE NETWORK GIVEN:\n",
    "        # - kernel size 5x5\n",
    "        # - the number of kernels: 128\n",
    "        # What is the value of in_channels here?\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, \n",
    "                               out_channels=128,\n",
    "                               kernel_size=(5,5), \n",
    "                               bias=False)\n",
    "\n",
    "        # FILLING THE ... TO COMPLETE THE 3ST 2D CONV LAYER OF THE NETWORK GIVEN:\n",
    "        # - kernel size 3x3\n",
    "        # - the number of kernels: 128\n",
    "        # What is the value of in_channels here?\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, \n",
    "                               out_channels=128,\n",
    "                               kernel_size=(3,3), \n",
    "                               bias=False)\n",
    "\n",
    "        # FILLING THE ... TO COMPLETE THE FOLLOWING FULLY CONNECTED (DENSE) LAYER OF THE NETWORK GIVEN:\n",
    "        # - the number of hidden units: 128\n",
    "        # What is the value of in_features here? Hint, you need to work out the number of features after the last convolutional layer\n",
    "        # Ref: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "        self.fc1 = nn.Linear(256, \n",
    "                             128, \n",
    "                             bias=True)\n",
    "        self.fc2 = nn.Linear(128, 1, bias=True)\n",
    "\n",
    "        # batch normalisation layers\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # call to initialise the network's weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "\n",
    "        init_layer(self.conv1)\n",
    "        init_layer(self.conv2)\n",
    "        init_layer(self.conv3)\n",
    "        init_layer(self.fc1)\n",
    "\n",
    "        init_bn(self.bn1)\n",
    "        init_bn(self.bn2)\n",
    "        init_bn(self.bn3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        (_, time_len, mel_bins) = x.shape\n",
    "        # reshape the input into 4D format (batch_size, channels, time_len, frequency_len)\n",
    "        \n",
    "        # Input\n",
    "        # torch.Size([4, 1, 400, 64])\n",
    "        # Conv1\n",
    "        # torch.Size([4, 64, 396, 60])\n",
    "        # Pool1\n",
    "        # torch.Size([4, 64, 50, 15])\n",
    "        # Conv2\n",
    "        # torch.Size([4, 128, 46, 11])\n",
    "        # Pool2\n",
    "        # torch.Size([4, 128, 6, 3])\n",
    "        # Conv3\n",
    "        # torch.Size([4, 128, 4, 1])\n",
    "        # Pool3\n",
    "        # torch.Size([4, 128, 2, 1])\n",
    "        x = x.view(-1, 1, time_len, mel_bins)\n",
    "        # print('Input')\n",
    "        # print(x.size())\n",
    "\n",
    "        # 1st conv layer + batch norm + relu activation\n",
    "        # QUESTION: WHAT IS THE SHAPE OF x AFTER THIS LINE?\n",
    "        # Note: the default stride is 1x1\n",
    "        x = nnF.relu(self.bn1(self.conv1(x)))\n",
    "        # print('Conv1')\n",
    "        # print(x.size())\n",
    "        \n",
    "        # max pooling with kernel size (8, 4)\n",
    "        # QUESTION: WHAT IS THE SHAPE OF x AFTER THIS LINE?\n",
    "        # QUESTION: WHAT IS THE EFFECT OF PADDING HERE?\n",
    "        x = nnF.max_pool2d(x,kernel_size=(8,4),padding=(4,0))\n",
    "        # print('Pool1')\n",
    "        # print(x.size())\n",
    "        \n",
    "        # 2nd conv layer + batch norm + relu activation\n",
    "        # QUESTION: WHAT IS THE SHAPE OF x AFTER THIS LINE?\n",
    "        # Note: the default stride is 1x1\n",
    "        x = nnF.relu(self.bn2(self.conv2(x)))\n",
    "        # print('Conv2')\n",
    "        # print(x.size())\n",
    "        \n",
    "        # max pooling with kernel size (8, 4)\n",
    "        # QUESTION: WHAT IS THE SHAPE OF x AFTER THIS LINE?\n",
    "        x = nnF.max_pool2d(x,kernel_size=(8,4),padding=(2,1))\n",
    "        # print('Pool2')\n",
    "        # print(x.size())\n",
    "        \n",
    "        # 3rd conv layer + batch norm + relu activation\n",
    "        # QUESTION: WHAT IS THE SHAPE OF x AFTER THIS LINE?\n",
    "        # Note: the default stride is 1x1\n",
    "        x = nnF.relu(self.bn3(self.conv3(x)))\n",
    "        # print('Conv3')\n",
    "        # print(x.size())\n",
    "        \n",
    "        # max pooling with kernel size (2, 1)\n",
    "        # QUESTION: WHAT IS THE SHAPE OF x AFTER THIS LINE?\n",
    "        x = nnF.max_pool2d(x,kernel_size=(2,1))\n",
    "        # print('Pool3')\n",
    "        # print(x.size())\n",
    "        \n",
    "        # flatten the feature map into a vector\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        # the first dense layer + relu activation\n",
    "        x = nnF.relu(self.fc1(x))\n",
    "        # the first dense layer + sigmoid activation\n",
    "        # QUESTION: WHY DO WE NEED TO USE SIGMOID ACTIVATION HERE?\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_and_convert(self, x):\n",
    "        \"Handles the torch<--->numpy tensor conversion, for convenience\"\n",
    "        x_torch = torch.FloatTensor(x)\n",
    "        x_torch = x_torch.to(device)\n",
    "        y_torch = self.forward(x_torch)\n",
    "        return y_torch.detach().cpu().numpy()\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CnnModel(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "  (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create a model instance\n",
    "net = CnnModel()\n",
    "net.to(device) \n",
    "print(net)\n",
    "\n",
    "# Binary-cross entropy loss, closely related to logistic regression loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Adam Optimizer, learning rate 0.001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minibatch size (remember stochastic gradient descent?)\n",
    "batch_size = 4\n",
    "\n",
    "# some helpful functions\n",
    "\n",
    "'''\n",
    "Evaluate a network \"model\" on the data \"data\" \n",
    "Predicted class labels will be returned\n",
    "'''\n",
    "def evaluate(model, data):\n",
    "    pred = np.zeros(len(data)) # for storing predicted class labels, one for each data sample\n",
    "    num_batch = len(data)//batch_size # number of batches in one data epoch\n",
    "    # evaluate batch by batch and store the output to \"pred\"\n",
    "    for i in range(num_batch):\n",
    "        temp = model.forward_and_convert(data[i*num_batch : (i+1)*num_batch])\n",
    "        pred[i*num_batch : (i+1)*num_batch] = temp.squeeze()\n",
    "    # some trailing data samples\n",
    "    if(num_batch*batch_size < len(data)):\n",
    "        temp = model.forward_and_convert(data[num_batch*batch_size :])\n",
    "        pred[num_batch*batch_size :] = temp.squeeze()\n",
    "    # each element in \"pred\" is the output after sigmoid function and has value in [0, 1].\n",
    "    # to obtain the discrete label (0 or 1 in this case), we threshold the value by 0.5.\n",
    "    pred[pred >= 0.5] = 1.\n",
    "    pred[pred < 0.5] = 0.\n",
    "    return pred\n",
    "\n",
    "'''\n",
    "Randomly shuffle the data. It will be used to shuffle the training data after every training epoch\n",
    "'''\n",
    "def shuffle_data(data, label):\n",
    "    # permute the data indices\n",
    "    rand_ind = np.random.permutation(len(data))\n",
    "    # re-order the data with the pumuted indices\n",
    "    return data[rand_ind], label[rand_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] loss: 0.24927278\n",
      "Validation accuracy: 0.6\n",
      "Saving best model\n",
      "[1] loss: 0.12614582\n",
      "Validation accuracy: 0.7\n",
      "Saving best model\n",
      "[2] loss: 0.32391205\n",
      "Validation accuracy: 0.7\n",
      "[3] loss: 0.18210995\n",
      "Validation accuracy: 0.6\n",
      "[4] loss: 0.04793742\n",
      "Validation accuracy: 0.6\n",
      "[5] loss: 0.11306479\n",
      "Validation accuracy: 0.6\n",
      "[6] loss: 1.19449663\n",
      "Validation accuracy: 0.7\n",
      "[7] loss: 0.13807926\n",
      "Validation accuracy: 0.6\n",
      "[8] loss: 0.04213774\n",
      "Validation accuracy: 0.7\n",
      "[9] loss: 0.56237787\n",
      "Validation accuracy: 0.7\n",
      "[10] loss: 0.20170890\n",
      "Validation accuracy: 0.7\n",
      "[11] loss: 0.19713144\n",
      "Validation accuracy: 0.7\n",
      "[12] loss: 0.22663447\n",
      "Validation accuracy: 0.7\n",
      "[13] loss: 0.19084458\n",
      "Validation accuracy: 0.7\n",
      "[14] loss: 0.07179625\n",
      "Validation accuracy: 0.6\n",
      "[15] loss: 0.08706286\n",
      "Validation accuracy: 0.6\n",
      "[16] loss: 0.11085810\n",
      "Validation accuracy: 0.7\n",
      "[17] loss: 0.13801363\n",
      "Validation accuracy: 0.7\n",
      "[18] loss: 0.04696465\n",
      "Validation accuracy: 0.7\n",
      "[19] loss: 0.01684972\n",
      "Validation accuracy: 0.7\n",
      "[20] loss: 0.07484926\n",
      "Validation accuracy: 0.7\n",
      "[21] loss: 0.67263865\n",
      "Validation accuracy: 0.6\n",
      "[22] loss: 0.01111780\n",
      "Validation accuracy: 0.7\n",
      "[23] loss: 0.49767739\n",
      "Validation accuracy: 0.6\n",
      "[24] loss: 0.01245093\n",
      "Validation accuracy: 0.6\n",
      "[25] loss: 0.44170755\n",
      "Validation accuracy: 0.6\n",
      "[26] loss: 0.04742536\n",
      "Validation accuracy: 0.6\n",
      "[27] loss: 0.06069858\n",
      "Validation accuracy: 0.7\n",
      "[28] loss: 0.03088340\n",
      "Validation accuracy: 0.7\n",
      "[29] loss: 0.09305619\n",
      "Validation accuracy: 0.6\n",
      "[30] loss: 0.09487922\n",
      "Validation accuracy: 0.6\n",
      "[31] loss: 0.32524931\n",
      "Validation accuracy: 0.6\n",
      "[32] loss: 0.40802863\n",
      "Validation accuracy: 0.7\n",
      "[33] loss: 0.45673776\n",
      "Validation accuracy: 0.6\n",
      "[34] loss: 0.14449848\n",
      "Validation accuracy: 0.6\n",
      "[35] loss: 0.01209344\n",
      "Validation accuracy: 0.6\n",
      "[36] loss: 0.02423986\n",
      "Validation accuracy: 0.6\n",
      "[37] loss: 0.00609660\n",
      "Validation accuracy: 0.6\n",
      "[38] loss: 1.04202020\n",
      "Validation accuracy: 0.6\n",
      "[39] loss: 0.00864990\n",
      "Validation accuracy: 0.6\n",
      "[40] loss: 0.17160597\n",
      "Validation accuracy: 0.5\n",
      "[41] loss: 0.01231239\n",
      "Validation accuracy: 0.5\n",
      "[42] loss: 0.08087435\n",
      "Validation accuracy: 0.6\n",
      "[43] loss: 0.08505616\n",
      "Validation accuracy: 0.7\n",
      "[44] loss: 0.00162043\n",
      "Validation accuracy: 0.6\n",
      "[45] loss: 0.00354696\n",
      "Validation accuracy: 0.6\n",
      "[46] loss: 0.00088793\n",
      "Validation accuracy: 0.6\n",
      "[47] loss: 0.00203984\n",
      "Validation accuracy: 0.6\n",
      "[48] loss: 0.01219617\n",
      "Validation accuracy: 0.6\n",
      "[49] loss: 0.09811941\n",
      "Validation accuracy: 0.6\n",
      "[50] loss: 0.04541299\n",
      "Validation accuracy: 0.6\n",
      "[51] loss: 0.02016504\n",
      "Validation accuracy: 0.6\n",
      "[52] loss: 0.28990072\n",
      "Validation accuracy: 0.6\n",
      "[53] loss: 0.08471356\n",
      "Validation accuracy: 0.5\n",
      "[54] loss: 0.00033999\n",
      "Validation accuracy: 0.6\n",
      "[55] loss: 0.00988647\n",
      "Validation accuracy: 0.6\n",
      "[56] loss: 0.33543444\n",
      "Validation accuracy: 0.7\n",
      "[57] loss: 0.00925045\n",
      "Validation accuracy: 0.6\n",
      "[58] loss: 0.02655480\n",
      "Validation accuracy: 0.6\n",
      "[59] loss: 0.36613053\n",
      "Validation accuracy: 0.6\n",
      "[60] loss: 0.03752201\n",
      "Validation accuracy: 0.6\n",
      "[61] loss: 0.01172059\n",
      "Validation accuracy: 0.6\n",
      "[62] loss: 0.00093431\n",
      "Validation accuracy: 0.6\n",
      "[63] loss: 0.26782295\n",
      "Validation accuracy: 0.7\n",
      "[64] loss: 0.02921397\n",
      "Validation accuracy: 0.7\n",
      "[65] loss: 0.06019261\n",
      "Validation accuracy: 0.7\n",
      "[66] loss: 0.05326678\n",
      "Validation accuracy: 0.7\n",
      "[67] loss: 0.08845356\n",
      "Validation accuracy: 0.7\n",
      "[68] loss: 0.07953846\n",
      "Validation accuracy: 0.7\n",
      "[69] loss: 0.00303877\n",
      "Validation accuracy: 0.6\n",
      "[70] loss: 0.03614859\n",
      "Validation accuracy: 0.7\n",
      "[71] loss: 0.90576184\n",
      "Validation accuracy: 0.7\n",
      "[72] loss: 0.09539273\n",
      "Validation accuracy: 0.5\n",
      "[73] loss: 0.00322583\n",
      "Validation accuracy: 0.6\n",
      "[74] loss: 0.01761591\n",
      "Validation accuracy: 0.6\n",
      "[75] loss: 0.01443342\n",
      "Validation accuracy: 0.7\n",
      "[76] loss: 0.02767619\n",
      "Validation accuracy: 0.6\n",
      "[77] loss: 0.00343904\n",
      "Validation accuracy: 0.6\n",
      "[78] loss: 0.35098618\n",
      "Validation accuracy: 0.7\n",
      "[79] loss: 0.02016413\n",
      "Validation accuracy: 0.6\n",
      "[80] loss: 0.72448969\n",
      "Validation accuracy: 0.7\n",
      "[81] loss: 0.11473827\n",
      "Validation accuracy: 0.7\n",
      "[82] loss: 0.00020817\n",
      "Validation accuracy: 0.7\n",
      "[83] loss: 0.02702231\n",
      "Validation accuracy: 0.6\n",
      "[84] loss: 0.00214864\n",
      "Validation accuracy: 0.7\n",
      "[85] loss: 0.00254055\n",
      "Validation accuracy: 0.7\n",
      "[86] loss: 0.04516378\n",
      "Validation accuracy: 0.7\n",
      "[87] loss: 0.34881562\n",
      "Validation accuracy: 0.7\n",
      "[88] loss: 0.00874392\n",
      "Validation accuracy: 0.6\n",
      "[89] loss: 0.49951062\n",
      "Validation accuracy: 0.6\n",
      "[90] loss: 0.00507659\n",
      "Validation accuracy: 0.6\n",
      "[91] loss: 0.16993631\n",
      "Validation accuracy: 0.6\n",
      "[92] loss: 0.00063970\n",
      "Validation accuracy: 0.6\n",
      "[93] loss: 0.00097791\n",
      "Validation accuracy: 0.5\n",
      "[94] loss: 0.27909148\n",
      "Validation accuracy: 0.5\n",
      "[95] loss: 0.01122387\n",
      "Validation accuracy: 0.5\n",
      "[96] loss: 0.03403745\n",
      "Validation accuracy: 0.5\n",
      "[97] loss: 0.01779463\n",
      "Validation accuracy: 0.6\n",
      "[98] loss: 0.01352365\n",
      "Validation accuracy: 0.6\n",
      "[99] loss: 0.02175793\n",
      "Validation accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "'''The training loop'''\n",
    "\n",
    "num_epochs = 100 # the number of training epoch (i.e. when you've gone through all samples of the training data, that's one epoch)\n",
    "evaluate_every_epoch = 1 # how often you want to evaluate the network during training?\n",
    "best_valid_acc = 0.0 # for keeping track of the best accuracy on the validation data\n",
    "saved_model = './best_model' # path for saving the best model during training\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # shuffle training data\n",
    "    train_data, train_label = shuffle_data(train_data, train_label)\n",
    "    \n",
    "    # the number of minibatch in one epoch\n",
    "    num_batch = len(train_data) // batch_size\n",
    "    for i in range(num_batch):\n",
    "        # sample one minibatch\n",
    "        # FILLING ... TO COMPLETE THE LINES BELOW TO SAMPLE THE I-TH MINIBATCH OF DATA FOR TRAINING\n",
    "        # Hint: you need to think about the starting and ending index of this minibatch in 'train_data' and 'train_label'\n",
    "        batch_data = train_data[i: i+batch_size]\n",
    "        label_data = train_label[i: i+batch_size]\n",
    "    \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(torch.FloatTensor(batch_data).to(device))\n",
    "        loss = criterion(outputs.squeeze(), torch.FloatTensor(label_data).to(device))\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss = loss.item()\n",
    "    # print training loss\n",
    "    print('[%d] loss: %.8f' %(epoch, running_loss))\n",
    "    \n",
    "    # evaluate the network on the validation data\n",
    "    if((epoch+1) % evaluate_every_epoch == 0):\n",
    "        valid_pred = evaluate(net, valid_data)\n",
    "        valid_acc = accuracy_score(valid_pred, valid_label)\n",
    "        print('Validation accuracy: %g' % valid_acc)\n",
    "        \n",
    "        # if the best validation performance so far, save the network to file \n",
    "        if(best_valid_acc < valid_acc):\n",
    "            print('Saving best model')\n",
    "            best_valid_acc = valid_acc\n",
    "            # COMPLETE THE LINE BELOW TO SAVE THE CURRENT BEST MODEL.\n",
    "            # - the path of the model is in the variable 'saved_model'\n",
    "            # Ref: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "            torch.save(net.state_dict(), saved_model)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. 1. 0. 0. 0. 0. 1. 1.]\n",
      "Test accuracy: 0.6\n",
      "Confusion_matrix\n",
      " [[2 1]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "'''When you are here, we have the best model saved in file.'''\n",
    "'''Then, load the saved model, and evaluate it on the test data'''\n",
    "net = CnnModel()\n",
    "# COMPLETE THE LINE BELOW TO LOAD THE SAVED MODEL.\n",
    "# - the path of the model is in the variable 'saved_model'\n",
    "# Ref: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "net.load_state_dict(torch.load(saved_model))\n",
    "net.eval()\n",
    "net.to(device)\n",
    "\n",
    "# evaluate on the test data\n",
    "test_pred = evaluate(net, test_data) \n",
    "print(test_pred)\n",
    "\n",
    "# test accuracy\n",
    "test_acc = accuracy_score(test_pred, test_label)\n",
    "print('Test accuracy: %g' % test_acc)\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(test_label, test_pred)\n",
    "print('Confusion_matrix\\n',confusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
